{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c598130",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     16\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bedfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationMetrics:\n",
    "    \"\"\"Comprehensive segmentation evaluation metrics\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-7  # Small value to avoid division by zero\n",
    "\n",
    "    def iou_score(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (IoU) score\n",
    "        Args:\n",
    "            pred_mask: Predicted binary mask (0s and 1s)\n",
    "            true_mask: Ground truth binary mask (0s and 1s)\n",
    "        Returns:\n",
    "            IoU score (float)\n",
    "        \"\"\"\n",
    "        # Ensure masks are binary\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "        true_mask = (true_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "        union = np.logical_or(pred_mask, true_mask).sum()\n",
    "\n",
    "        if union == 0:\n",
    "            return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "        iou = intersection / (union + self.epsilon)\n",
    "        return float(iou)\n",
    "\n",
    "    def dice_score(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate Dice coefficient (F1-score for segmentation)\n",
    "        Args:\n",
    "            pred_mask: Predicted binary mask (0s and 1s)\n",
    "            true_mask: Ground truth binary mask (0s and 1s)\n",
    "        Returns:\n",
    "            Dice score (float)\n",
    "        \"\"\"\n",
    "        # Ensure masks are binary\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "        true_mask = (true_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Calculate intersection\n",
    "        intersection = np.logical_and(pred_mask, true_mask).sum()\n",
    "        total_pixels = pred_mask.sum() + true_mask.sum()\n",
    "\n",
    "        if total_pixels == 0:\n",
    "            return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "        dice = (2.0 * intersection) / (total_pixels + self.epsilon)\n",
    "        return float(dice)\n",
    "\n",
    "    def pixel_accuracy(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate pixel-wise accuracy\n",
    "        Args:\n",
    "            pred_mask: Predicted binary mask (0s and 1s)\n",
    "            true_mask: Ground truth binary mask (0s and 1s)\n",
    "        Returns:\n",
    "            Pixel accuracy (float)\n",
    "        \"\"\"\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "        true_mask = (true_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        correct_pixels = np.sum(pred_mask == true_mask)\n",
    "        total_pixels = pred_mask.size\n",
    "\n",
    "        return correct_pixels / total_pixels\n",
    "\n",
    "    def precision_recall_f1(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate precision, recall, and F1-score at pixel level\n",
    "        Args:\n",
    "            pred_mask: Predicted binary mask (0s and 1s)\n",
    "            true_mask: Ground truth binary mask (0s and 1s)\n",
    "        Returns:\n",
    "            Dictionary with precision, recall, f1 scores\n",
    "        \"\"\"\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8).flatten()\n",
    "        true_mask = (true_mask > 0.5).astype(np.uint8).flatten()\n",
    "\n",
    "        # Calculate confusion matrix elements\n",
    "        tp = np.sum((pred_mask == 1) & (true_mask == 1))\n",
    "        fp = np.sum((pred_mask == 1) & (true_mask == 0))\n",
    "        fn = np.sum((pred_mask == 0) & (true_mask == 1))\n",
    "        tn = np.sum((pred_mask == 0) & (true_mask == 0))\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp + self.epsilon)\n",
    "        recall = tp / (tp + fn + self.epsilon)\n",
    "        f1 = 2 * precision * recall / (precision + recall + self.epsilon)\n",
    "        specificity = tn / (tn + fp + self.epsilon)\n",
    "\n",
    "        return {\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"f1_score\": float(f1),\n",
    "            \"specificity\": float(specificity),\n",
    "            \"true_positives\": int(tp),\n",
    "            \"false_positives\": int(fp),\n",
    "            \"false_negatives\": int(fn),\n",
    "            \"true_negatives\": int(tn),\n",
    "        }\n",
    "\n",
    "    def hausdorff_distance(self, pred_mask, true_mask):\n",
    "        \"\"\"\n",
    "        Calculate Hausdorff distance between mask boundaries\n",
    "        Args:\n",
    "            pred_mask: Predicted binary mask (0s and 1s)\n",
    "            true_mask: Ground truth binary mask (0s and 1s)\n",
    "        Returns:\n",
    "            Hausdorff distance (float)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "            pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "            true_mask = (true_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            # Find contours/boundaries\n",
    "            pred_contours, _ = cv2.findContours(\n",
    "                pred_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            true_contours, _ = cv2.findContours(\n",
    "                true_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "\n",
    "            if not pred_contours or not true_contours:\n",
    "                return float(\"inf\")\n",
    "\n",
    "            # Get boundary points\n",
    "            pred_points = np.vstack(\n",
    "                [contour.reshape(-1, 2) for contour in pred_contours]\n",
    "            )\n",
    "            true_points = np.vstack(\n",
    "                [contour.reshape(-1, 2) for contour in true_contours]\n",
    "            )\n",
    "\n",
    "            # Calculate directed Hausdorff distances\n",
    "            hd1 = directed_hausdorff(pred_points, true_points)[0]\n",
    "            hd2 = directed_hausdorff(true_points, pred_points)[0]\n",
    "\n",
    "            return float(max(hd1, hd2))\n",
    "\n",
    "        except ImportError:\n",
    "            # Fallback to simple boundary distance if scipy not available\n",
    "            return self.simple_boundary_distance(pred_mask, true_mask)\n",
    "\n",
    "    def simple_boundary_distance(self, pred_mask, true_mask):\n",
    "        \"\"\"Simple boundary distance calculation without scipy\"\"\"\n",
    "        pred_mask = (pred_mask > 0.5).astype(np.uint8)\n",
    "        true_mask = (true_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "        # Calculate boundaries using morphological operations\n",
    "        kernel = np.ones((3, 3), np.uint8)\n",
    "        pred_boundary = cv2.morphologyEx(pred_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "        true_boundary = cv2.morphologyEx(true_mask, cv2.MORPH_GRADIENT, kernel)\n",
    "\n",
    "        if pred_boundary.sum() == 0 or true_boundary.sum() == 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        # Simple average distance between boundaries\n",
    "        pred_coords = np.column_stack(np.where(pred_boundary))\n",
    "        true_coords = np.column_stack(np.where(true_boundary))\n",
    "\n",
    "        distances = []\n",
    "        for pred_point in pred_coords:\n",
    "            min_dist = np.min(np.linalg.norm(true_coords - pred_point, axis=1))\n",
    "            distances.append(min_dist)\n",
    "\n",
    "        return float(np.mean(distances)) if distances else float(\"inf\")\n",
    "\n",
    "\n",
    "class YOLOSegmentationEvaluator:\n",
    "    \"\"\"Evaluator for YOLO segmentation model using IoU/Dice metrics\"\"\"\n",
    "\n",
    "    def __init__(self, model_path=None, device=None):\n",
    "        self.device = device if device is not None else DEFAULT_DEVICE\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        self.metrics_calculator = SegmentationMetrics()\n",
    "        self.results = []\n",
    "\n",
    "        # Class mappings from documentation\n",
    "        self.tool_classes = list(range(12))  # Classes 0-11\n",
    "        self.tissue_classes = list(range(12, 21))  # Classes 12-20\n",
    "        self.class_names = self.get_class_names()\n",
    "\n",
    "    def get_class_names(self):\n",
    "        \"\"\"Define class names based on documentation\"\"\"\n",
    "        # These would need to be filled in with actual class names from your dataset\n",
    "        tool_names = [f\"Tool_{i}\" for i in range(12)]\n",
    "        tissue_names = [f\"Tissue_{i}\" for i in range(9)]\n",
    "        return tool_names + tissue_names\n",
    "\n",
    "    def load_ground_truth_annotations(self, annotation_path):\n",
    "        \"\"\"\n",
    "        Load ground truth annotations\n",
    "        Expected format: YOLO segmentation format with polygon coordinates\n",
    "        \"\"\"\n",
    "        annotations = {}\n",
    "        annotation_path = Path(annotation_path)\n",
    "\n",
    "        if annotation_path.is_file():\n",
    "            # Single annotation file\n",
    "            with open(annotation_path, \"r\") as f:\n",
    "                annotations[annotation_path.stem] = f.readlines()\n",
    "        elif annotation_path.is_dir():\n",
    "            # Directory of annotation files\n",
    "            for ann_file in annotation_path.glob(\"*.txt\"):\n",
    "                with open(ann_file, \"r\") as f:\n",
    "                    annotations[ann_file.stem] = f.readlines()\n",
    "\n",
    "        return annotations\n",
    "\n",
    "    def parse_yolo_annotation(self, annotation_line, img_width, img_height):\n",
    "        \"\"\"\n",
    "        Parse YOLO segmentation annotation line\n",
    "        Format: class_id x1 y1 x2 y2 ... xn yn (normalized coordinates)\n",
    "        \"\"\"\n",
    "        parts = annotation_line.strip().split()\n",
    "        if len(parts) < 7:  # Need at least class + 3 points (6 coordinates)\n",
    "            return None, None\n",
    "\n",
    "        class_id = int(parts[0])\n",
    "        coords = list(map(float, parts[1:]))\n",
    "\n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        points = []\n",
    "        for i in range(0, len(coords), 2):\n",
    "            x = int(coords[i] * img_width)\n",
    "            y = int(coords[i + 1] * img_height)\n",
    "            points.append([x, y])\n",
    "\n",
    "        # Create mask from polygon\n",
    "        mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "        if len(points) >= 3:\n",
    "            cv2.fillPoly(mask, [np.array(points, dtype=np.int32)], 255)\n",
    "\n",
    "        return class_id, mask\n",
    "\n",
    "    def create_dummy_predictions(self, img_width, img_height, num_objects=3):\n",
    "        \"\"\"\n",
    "        Create dummy predictions for demonstration\n",
    "        Replace this with actual model inference\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(num_objects):\n",
    "            # Random class\n",
    "            class_id = np.random.choice(list(range(21)))\n",
    "\n",
    "            # Create random mask\n",
    "            mask = np.zeros((img_height, img_width), dtype=np.uint8)\n",
    "\n",
    "            # Random polygon\n",
    "            center_x = np.random.randint(img_width // 4, 3 * img_width // 4)\n",
    "            center_y = np.random.randint(img_height // 4, 3 * img_height // 4)\n",
    "            radius = np.random.randint(20, min(img_width, img_height) // 4)\n",
    "\n",
    "            # Create circular mask\n",
    "            cv2.circle(mask, (center_x, center_y), radius, 255, -1)\n",
    "\n",
    "            # Random confidence\n",
    "            confidence = np.random.uniform(0.5, 0.95)\n",
    "\n",
    "            predictions.append(\n",
    "                {\"class_id\": class_id, \"mask\": mask, \"confidence\": confidence}\n",
    "            )\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def evaluate_image(\n",
    "        self, image_path, annotation_lines, img_width=None, img_height=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate predictions vs ground truth for a single image\n",
    "        \"\"\"\n",
    "        image_path = Path(image_path)\n",
    "\n",
    "        # Load image to get dimensions if not provided\n",
    "        if img_width is None or img_height is None:\n",
    "            if image_path.exists():\n",
    "                img = cv2.imread(str(image_path))\n",
    "                img_height, img_width = img.shape[:2]\n",
    "            else:\n",
    "                # Default dimensions if image not found\n",
    "                img_width, img_height = 640, 640\n",
    "\n",
    "        # Parse ground truth annotations\n",
    "        gt_masks = {}  # class_id -> list of masks\n",
    "        for line in annotation_lines:\n",
    "            class_id, mask = self.parse_yolo_annotation(line, img_width, img_height)\n",
    "            if class_id is not None:\n",
    "                if class_id not in gt_masks:\n",
    "                    gt_masks[class_id] = []\n",
    "                gt_masks[class_id].append(mask)\n",
    "\n",
    "        # Get predictions (replace with actual model inference)\n",
    "        predictions = self.create_dummy_predictions(img_width, img_height)\n",
    "\n",
    "        # Evaluate each prediction against ground truth\n",
    "        image_results = {\n",
    "            \"image_name\": image_path.name,\n",
    "            \"image_width\": img_width,\n",
    "            \"image_height\": img_height,\n",
    "            \"class_results\": {},\n",
    "            \"overall_metrics\": {},\n",
    "        }\n",
    "\n",
    "        all_ious = []\n",
    "        all_dices = []\n",
    "        all_pixel_accuracies = []\n",
    "\n",
    "        for pred in predictions:\n",
    "            pred_class = pred[\"class_id\"]\n",
    "            pred_mask = pred[\"mask\"]\n",
    "            pred_confidence = pred[\"confidence\"]\n",
    "\n",
    "            if pred_class in gt_masks:\n",
    "                # Find best matching ground truth mask\n",
    "                best_iou = 0\n",
    "                best_metrics = None\n",
    "\n",
    "                for gt_mask in gt_masks[pred_class]:\n",
    "                    # Calculate metrics\n",
    "                    iou = self.metrics_calculator.iou_score(pred_mask, gt_mask)\n",
    "                    dice = self.metrics_calculator.dice_score(pred_mask, gt_mask)\n",
    "                    pixel_acc = self.metrics_calculator.pixel_accuracy(\n",
    "                        pred_mask, gt_mask\n",
    "                    )\n",
    "                    prf_metrics = self.metrics_calculator.precision_recall_f1(\n",
    "                        pred_mask, gt_mask\n",
    "                    )\n",
    "                    hausdorff = self.metrics_calculator.hausdorff_distance(\n",
    "                        pred_mask, gt_mask\n",
    "                    )\n",
    "\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_metrics = {\n",
    "                            \"class_id\": pred_class,\n",
    "                            \"class_name\": self.class_names[pred_class],\n",
    "                            \"confidence\": pred_confidence,\n",
    "                            \"iou\": iou,\n",
    "                            \"dice\": dice,\n",
    "                            \"pixel_accuracy\": pixel_acc,\n",
    "                            \"hausdorff_distance\": hausdorff,\n",
    "                            **prf_metrics,\n",
    "                        }\n",
    "\n",
    "                if best_metrics:\n",
    "                    if pred_class not in image_results[\"class_results\"]:\n",
    "                        image_results[\"class_results\"][pred_class] = []\n",
    "                    image_results[\"class_results\"][pred_class].append(best_metrics)\n",
    "\n",
    "                    all_ious.append(best_metrics[\"iou\"])\n",
    "                    all_dices.append(best_metrics[\"dice\"])\n",
    "                    all_pixel_accuracies.append(best_metrics[\"pixel_accuracy\"])\n",
    "\n",
    "        # Calculate overall image metrics\n",
    "        if all_ious:\n",
    "            image_results[\"overall_metrics\"] = {\n",
    "                \"mean_iou\": np.mean(all_ious),\n",
    "                \"mean_dice\": np.mean(all_dices),\n",
    "                \"mean_pixel_accuracy\": np.mean(all_pixel_accuracies),\n",
    "                \"num_predictions\": len(predictions),\n",
    "                \"num_matched\": len(all_ious),\n",
    "                \"match_rate\": len(all_ious) / len(predictions) if predictions else 0,\n",
    "            }\n",
    "\n",
    "        return image_results\n",
    "\n",
    "    def evaluate_dataset(\n",
    "        self, images_dir, annotations_dir, output_dir=\"evaluation_results\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Evaluate entire dataset\n",
    "        \"\"\"\n",
    "        images_dir = Path(images_dir)\n",
    "        annotations_dir = Path(annotations_dir)\n",
    "        output_dir = Path(output_dir)\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        print(\"Starting segmentation evaluation...\")\n",
    "        print(f\"Images directory: {images_dir}\")\n",
    "        print(f\"Annotations directory: {annotations_dir}\")\n",
    "\n",
    "        # Find image files\n",
    "        image_extensions = [\".jpg\", \".jpeg\", \".png\", \".bmp\"]\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(images_dir.glob(f\"*{ext}\"))\n",
    "\n",
    "        print(f\"Found {len(image_files)} images\")\n",
    "\n",
    "        # Load annotations\n",
    "        annotations = self.load_ground_truth_annotations(annotations_dir)\n",
    "        print(f\"Loaded annotations for {len(annotations)} files\")\n",
    "\n",
    "        # Evaluate each image\n",
    "        all_results = []\n",
    "\n",
    "        for image_file in image_files:\n",
    "            image_stem = image_file.stem\n",
    "\n",
    "            if image_stem in annotations:\n",
    "                print(f\"Evaluating: {image_file.name}\")\n",
    "\n",
    "                try:\n",
    "                    result = self.evaluate_image(image_file, annotations[image_stem])\n",
    "                    all_results.append(result)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating {image_file.name}: {e}\")\n",
    "            else:\n",
    "                print(f\"No annotation found for: {image_file.name}\")\n",
    "\n",
    "        # Generate comprehensive report\n",
    "        if all_results:\n",
    "            self.generate_evaluation_report(all_results, output_dir)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "    def generate_evaluation_report(self, all_results, output_dir):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        output_dir = Path(output_dir)\n",
    "\n",
    "        # Compile statistics\n",
    "        overall_stats = {\n",
    "            \"total_images\": len(all_results),\n",
    "            \"mean_iou\": [],\n",
    "            \"mean_dice\": [],\n",
    "            \"mean_pixel_accuracy\": [],\n",
    "            \"class_performance\": {},\n",
    "        }\n",
    "\n",
    "        # Per-image summary data\n",
    "        summary_data = []\n",
    "        detailed_data = []\n",
    "\n",
    "        for result in all_results:\n",
    "            if \"overall_metrics\" in result and result[\"overall_metrics\"]:\n",
    "                metrics = result[\"overall_metrics\"]\n",
    "                overall_stats[\"mean_iou\"].append(metrics[\"mean_iou\"])\n",
    "                overall_stats[\"mean_dice\"].append(metrics[\"mean_dice\"])\n",
    "                overall_stats[\"mean_pixel_accuracy\"].append(\n",
    "                    metrics[\"mean_pixel_accuracy\"]\n",
    "                )\n",
    "\n",
    "                summary_data.append(\n",
    "                    {\n",
    "                        \"Image\": result[\"image_name\"],\n",
    "                        \"Width\": result[\"image_width\"],\n",
    "                        \"Height\": result[\"image_height\"],\n",
    "                        \"Mean IoU\": metrics[\"mean_iou\"],\n",
    "                        \"Mean Dice\": metrics[\"mean_dice\"],\n",
    "                        \"Mean Pixel Accuracy\": metrics[\"mean_pixel_accuracy\"],\n",
    "                        \"Predictions\": metrics[\"num_predictions\"],\n",
    "                        \"Matched\": metrics[\"num_matched\"],\n",
    "                        \"Match Rate\": metrics[\"match_rate\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Collect detailed per-class results\n",
    "            for class_id, class_results in result[\"class_results\"].items():\n",
    "                for class_result in class_results:\n",
    "                    detailed_data.append(\n",
    "                        {\n",
    "                            \"Image\": result[\"image_name\"],\n",
    "                            \"Class ID\": class_id,\n",
    "                            \"Class Name\": class_result[\"class_name\"],\n",
    "                            \"Confidence\": class_result[\"confidence\"],\n",
    "                            \"IoU\": class_result[\"iou\"],\n",
    "                            \"Dice\": class_result[\"dice\"],\n",
    "                            \"Pixel Accuracy\": class_result[\"pixel_accuracy\"],\n",
    "                            \"Precision\": class_result[\"precision\"],\n",
    "                            \"Recall\": class_result[\"recall\"],\n",
    "                            \"F1 Score\": class_result[\"f1_score\"],\n",
    "                            \"Specificity\": class_result[\"specificity\"],\n",
    "                            \"Hausdorff Distance\": class_result[\"hausdorff_distance\"],\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        # Save CSV files\n",
    "        if summary_data:\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            summary_df.to_csv(output_dir / \"evaluation_summary.csv\", index=False)\n",
    "\n",
    "        if detailed_data:\n",
    "            detailed_df = pd.DataFrame(detailed_data)\n",
    "            detailed_df.to_csv(output_dir / \"detailed_evaluation.csv\", index=False)\n",
    "\n",
    "            # Class-wise performance\n",
    "            class_stats = (\n",
    "                detailed_df.groupby([\"Class ID\", \"Class Name\"])\n",
    "                .agg(\n",
    "                    {\n",
    "                        \"IoU\": [\"mean\", \"std\", \"min\", \"max\", \"count\"],\n",
    "                        \"Dice\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "                        \"Pixel Accuracy\": [\"mean\", \"std\", \"min\", \"max\"],\n",
    "                        \"Precision\": [\"mean\", \"std\"],\n",
    "                        \"Recall\": [\"mean\", \"std\"],\n",
    "                        \"F1 Score\": [\"mean\", \"std\"],\n",
    "                    }\n",
    "                )\n",
    "                .round(4)\n",
    "            )\n",
    "\n",
    "            class_stats.to_csv(output_dir / \"class_performance.csv\")\n",
    "\n",
    "        # Generate text report\n",
    "        self.generate_text_report(\n",
    "            overall_stats, summary_data, detailed_data, output_dir\n",
    "        )\n",
    "\n",
    "        # Generate visualizations\n",
    "        self.generate_visualizations(summary_data, detailed_data, output_dir)\n",
    "\n",
    "        print(f\"\\n✅ Evaluation complete!\")\n",
    "        print(f\"📊 Results saved to: {output_dir}\")\n",
    "        print(f\"📈 Summary: {len(all_results)} images evaluated\")\n",
    "\n",
    "        if overall_stats[\"mean_iou\"]:\n",
    "            print(f\"📊 Overall Mean IoU: {np.mean(overall_stats['mean_iou']):.4f}\")\n",
    "            print(f\"📊 Overall Mean Dice: {np.mean(overall_stats['mean_dice']):.4f}\")\n",
    "\n",
    "    def generate_text_report(\n",
    "        self, overall_stats, summary_data, detailed_data, output_dir\n",
    "    ):\n",
    "        \"\"\"Generate detailed text report\"\"\"\n",
    "        report_path = output_dir / \"evaluation_report.txt\"\n",
    "\n",
    "        with open(report_path, \"w\") as f:\n",
    "            f.write(\"SEGMENTATION EVALUATION REPORT\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "            f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "            # Overall Statistics\n",
    "            f.write(\"OVERALL STATISTICS\\n\")\n",
    "            f.write(\"-\" * 20 + \"\\n\")\n",
    "            f.write(f\"Total Images Evaluated: {overall_stats['total_images']}\\n\")\n",
    "\n",
    "            if overall_stats[\"mean_iou\"]:\n",
    "                f.write(\n",
    "                    f\"Dataset Mean IoU: {np.mean(overall_stats['mean_iou']):.4f} ± {np.std(overall_stats['mean_iou']):.4f}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"Dataset Mean Dice: {np.mean(overall_stats['mean_dice']):.4f} ± {np.std(overall_stats['mean_dice']):.4f}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"Dataset Mean Pixel Accuracy: {np.mean(overall_stats['mean_pixel_accuracy']):.4f} ± {np.std(overall_stats['mean_pixel_accuracy']):.4f}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"IoU Range: {np.min(overall_stats['mean_iou']):.4f} - {np.max(overall_stats['mean_iou']):.4f}\\n\"\n",
    "                )\n",
    "                f.write(\n",
    "                    f\"Dice Range: {np.min(overall_stats['mean_dice']):.4f} - {np.max(overall_stats['mean_dice']):.4f}\\n\"\n",
    "                )\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            # Per-image results\n",
    "            if summary_data:\n",
    "                f.write(\"PER-IMAGE RESULTS\\n\")\n",
    "                f.write(\"-\" * 17 + \"\\n\")\n",
    "                for i, data in enumerate(summary_data[:10], 1):  # Show first 10\n",
    "                    f.write(f\"{i}. {data['Image']}\\n\")\n",
    "                    f.write(f\"   Resolution: {data['Width']}x{data['Height']}\\n\")\n",
    "                    f.write(f\"   Mean IoU: {data['Mean IoU']:.4f}\\n\")\n",
    "                    f.write(f\"   Mean Dice: {data['Mean Dice']:.4f}\\n\")\n",
    "                    f.write(\n",
    "                        f\"   Predictions: {data['Predictions']} (Matched: {data['Matched']})\\n\\n\"\n",
    "                    )\n",
    "\n",
    "                if len(summary_data) > 10:\n",
    "                    f.write(f\"... and {len(summary_data) - 10} more images\\n\\n\")\n",
    "\n",
    "            # Class performance summary\n",
    "            if detailed_data:\n",
    "                f.write(\"CLASS PERFORMANCE SUMMARY\\n\")\n",
    "                f.write(\"-\" * 25 + \"\\n\")\n",
    "                detailed_df = pd.DataFrame(detailed_data)\n",
    "                class_summary = (\n",
    "                    detailed_df.groupby(\"Class Name\")\n",
    "                    .agg(\n",
    "                        {\n",
    "                            \"IoU\": \"mean\",\n",
    "                            \"Dice\": \"mean\",\n",
    "                            \"Precision\": \"mean\",\n",
    "                            \"Recall\": \"mean\",\n",
    "                            \"F1 Score\": \"mean\",\n",
    "                        }\n",
    "                    )\n",
    "                    .round(4)\n",
    "                )\n",
    "\n",
    "                for class_name, metrics in class_summary.iterrows():\n",
    "                    f.write(f\"{class_name}:\\n\")\n",
    "                    f.write(f\"  IoU: {metrics['IoU']:.4f}\\n\")\n",
    "                    f.write(f\"  Dice: {metrics['Dice']:.4f}\\n\")\n",
    "                    f.write(f\"  Precision: {metrics['Precision']:.4f}\\n\")\n",
    "                    f.write(f\"  Recall: {metrics['Recall']:.4f}\\n\")\n",
    "                    f.write(f\"  F1: {metrics['F1 Score']:.4f}\\n\\n\")\n",
    "\n",
    "    def generate_visualizations(self, summary_data, detailed_data, output_dir):\n",
    "        \"\"\"Generate evaluation visualizations\"\"\"\n",
    "        if not summary_data and not detailed_data:\n",
    "            return\n",
    "\n",
    "        plt.style.use(\"default\")\n",
    "\n",
    "        # 1. Overall metrics distribution\n",
    "        if summary_data:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "            # IoU distribution\n",
    "            axes[0, 0].hist(summary_df[\"Mean IoU\"], bins=20, alpha=0.7, color=\"blue\")\n",
    "            axes[0, 0].set_title(\"IoU Score Distribution\")\n",
    "            axes[0, 0].set_xlabel(\"Mean IoU\")\n",
    "            axes[0, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "            # Dice distribution\n",
    "            axes[0, 1].hist(summary_df[\"Mean Dice\"], bins=20, alpha=0.7, color=\"green\")\n",
    "            axes[0, 1].set_title(\"Dice Score Distribution\")\n",
    "            axes[0, 1].set_xlabel(\"Mean Dice\")\n",
    "            axes[0, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "            # Pixel accuracy distribution\n",
    "            axes[1, 0].hist(\n",
    "                summary_df[\"Mean Pixel Accuracy\"], bins=20, alpha=0.7, color=\"orange\"\n",
    "            )\n",
    "            axes[1, 0].set_title(\"Pixel Accuracy Distribution\")\n",
    "            axes[1, 0].set_xlabel(\"Mean Pixel Accuracy\")\n",
    "            axes[1, 0].set_ylabel(\"Frequency\")\n",
    "\n",
    "            # Match rate\n",
    "            axes[1, 1].hist(summary_df[\"Match Rate\"], bins=20, alpha=0.7, color=\"red\")\n",
    "            axes[1, 1].set_title(\"Prediction Match Rate Distribution\")\n",
    "            axes[1, 1].set_xlabel(\"Match Rate\")\n",
    "            axes[1, 1].set_ylabel(\"Frequency\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                output_dir / \"metrics_distribution.png\", dpi=300, bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "        # 2. Class-wise performance\n",
    "        if detailed_data:\n",
    "            detailed_df = pd.DataFrame(detailed_data)\n",
    "\n",
    "            # Class-wise IoU comparison\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            class_iou = (\n",
    "                detailed_df.groupby(\"Class Name\")[\"IoU\"]\n",
    "                .mean()\n",
    "                .sort_values(ascending=True)\n",
    "            )\n",
    "            class_iou.plot(kind=\"barh\", color=\"skyblue\")\n",
    "            plt.title(\"Mean IoU Score by Class\")\n",
    "            plt.xlabel(\"Mean IoU\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                output_dir / \"class_iou_comparison.png\", dpi=300, bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "            # Metrics correlation heatmap\n",
    "            numeric_cols = [\n",
    "                \"IoU\",\n",
    "                \"Dice\",\n",
    "                \"Pixel Accuracy\",\n",
    "                \"Precision\",\n",
    "                \"Recall\",\n",
    "                \"F1 Score\",\n",
    "            ]\n",
    "            correlation_matrix = detailed_df[numeric_cols].corr()\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", center=0)\n",
    "            plt.title(\"Metrics Correlation Matrix\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                output_dir / \"metrics_correlation.png\", dpi=300, bbox_inches=\"tight\"\n",
    "            )\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def run_yolo_model_evaluation(model_path, test_images_dir, annotations_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Run evaluation using actual YOLO model (placeholder for real implementation)\n",
    "    This would need to be integrated with your actual YOLO model\n",
    "    \"\"\"\n",
    "    print(\"🔄 This function would integrate with your actual YOLO segmentation model\")\n",
    "    print(\"🔄 For now, using dummy predictions for demonstration\")\n",
    "\n",
    "    # This is where you would load and run your actual YOLO model\n",
    "    # model = YOLO(model_path)  # YOLOv11 loading\n",
    "    # results = model.predict(test_images_dir)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate_tti_classifier_pixels(\n",
    "    classifier_model_path, test_rois_dir, ground_truth_dir, output_dir\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate TTI classifier at pixel level\n",
    "    This would compare predicted ROI classifications with ground truth masks\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    print(\"\\n🔄 TTI Classifier Pixel-Level Evaluation\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # This would need your actual TTI classifier and ROI data\n",
    "    # For now, creating demonstration structure\n",
    "\n",
    "    results = {\n",
    "        \"total_rois\": 0,\n",
    "        \"correct_classifications\": 0,\n",
    "        \"pixel_level_metrics\": {},\n",
    "        \"class_wise_performance\": {},\n",
    "    }\n",
    "\n",
    "    # Placeholder for actual implementation\n",
    "    print(\"🔄 This would evaluate your TTI classifier predictions against ground truth\")\n",
    "    print(\"🔄 Metrics would include:\")\n",
    "    print(\"   - Pixel-wise accuracy for ROI classification\")\n",
    "    print(\"   - Precision/Recall for TTI vs No-TTI regions\")\n",
    "    print(\"   - Boundary accuracy for interaction regions\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396aff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = \"./images\"\n",
    "annotations_dir = \"./annotations\"\n",
    "output_dir = \"./iou_dice_evaluation_results\"\n",
    "\n",
    "print(\"IoU/Dice Segmentation Evaluation\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = YOLOSegmentationEvaluator()\n",
    "\n",
    "# Check if directories exist\n",
    "if not Path(images_dir).exists():\n",
    "    print(f\"⚠️  Images directory not found: {images_dir}\")\n",
    "    exit()\n",
    "\n",
    "if not Path(annotations_dir).exists():\n",
    "    print(f\"⚠️  Annotations directory not found: {annotations_dir}\")\n",
    "    exit()\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.evaluate_dataset(images_dir, annotations_dir, output_dir)\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n✅ Evaluation completed successfully!\")\n",
    "    print(f\"📊 Evaluated {len(results)} images\")\n",
    "    print(f\"📁 Results saved to: {output_dir}\")\n",
    "else:\n",
    "    print(\"❌ No results generated. Check your image and annotation paths.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
